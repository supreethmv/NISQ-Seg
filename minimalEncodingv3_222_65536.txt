for layer_i in range(n_layers):
  if layer_i%2:
      for qubit_i in range(nq - 1):
          qc.cx(qubit_i, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
      qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__0000__2.793597__1__0.0__0.0002__40__COBYLA
4__2__2__True__0111__4.452715__2__-0.51__-3.0431__64__COBYLA
4__2__2__True__1101__7.001194__3__-0.62__-3.4666__103__COBYLA
4__2__2__True__1101__10.971132__4__-0.62__-2.4405__136__COBYLA
4__2__2__True__0111__11.683147__5__-0.51__-3.2058__166__COBYLA
16__4__4__True__1111111111111111__5.582096__1__0.0__-10.0812__62__COBYLA
16__4__4__True__1111111101101011__10.281882__2__-1.9__-14.111__118__COBYLA
16__4__4__True__1111111111111111__15.64858__3__0.0__-8.6571__178__COBYLA
16__4__4__True__1111001111110110__16.948075__4__-2.26__-11.0103__195__COBYLA
16__4__4__True__1111111111110111__53.215853__5__-1.25__-13.4538__263__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__4.459728__1__0__0__43__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__8.938483__2__0__0__85__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__27.648763__3__0.0__-102.62__261__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111110__30.886311__4__-1.04__-97.1924__284__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__50.019706__5__0.0__-99.4522__466__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  if layer_i%2:
      for qubit_i in range(nq - 1):
          qc.cx(0, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
      qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__0000__2.659788__1__0.0__0.0003__39__COBYLA
4__2__2__True__0111__5.398483__2__-0.51__-3.175__82__COBYLA
4__2__2__True__1101__7.951608__3__-0.62__-3.4273__119__COBYLA
4__2__2__True__1111__46.097798__4__0.0__-2.4348__149__COBYLA
4__2__2__True__0111__11.690116__5__-0.51__-3.0535__174__COBYLA
16__4__4__True__1111111111111111__26.155758__1__0.0__100000.0__55__COBYLA
16__4__4__True__1111111101111110__9.705774__2__-1.91__-12.3029__114__COBYLA
16__4__4__True__1110111111110111__12.779233__3__-0.27__-8.223__150__COBYLA
16__4__4__True__1111111101110110__19.11858__4__-2.04__-16.1958__224__COBYLA
16__4__4__True__1100000011000000__22.837192__5__0.41__-1.848__273__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__4.492834__1__0__0__43__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__18.069043__2__0.0__100000.0__173__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__23.188098__3__0.0__100000.0__220__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__16.81104__4__0__0__169__COBYLA
64__8__8__True__1111111111111111110111111111111111111111111111111111111111111111__44.552458__5__-0.79__-103.9423__417__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  for qubit_i in range(nq - 1):
    if layer_i%nq != (qubit_i + 1)%nq:
      qc.cx(layer_i%nq, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
    qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__0000__2.5248__1__0.0__0.0004__38__COBYLA
4__2__2__True__1100__7.421498__2__0.07__-0.7814__60__COBYLA
4__2__2__True__1100__5.941992__3__0.07__-0.8102__92__COBYLA
4__2__2__True__1101__8.852605__4__-0.62__-3.4409__135__COBYLA
4__2__2__True__1101__10.249594__5__-0.62__-3.514__155__COBYLA
16__4__4__True__1111111111111111__9.740423__1__0.0__100000.0__58__COBYLA
16__4__4__True__1111111111111111__9.56468__2__0.0__-10.08__113__COBYLA
16__4__4__True__1111111111110000__13.01836__3__-0.89__-10.6666__151__COBYLA
16__4__4__True__0011111111110000__16.889672__4__-0.16__-8.1102__199__COBYLA
16__4__4__True__1111111111110011__24.612387__5__0.0__-11.3128__283__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__4.38383__1__0__0__43__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__16.721019__2__0.0__-102.62__163__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__21.318358__3__0.0__-101.9974__203__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__36.44988__4__0.0__-102.5621__343__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__39.880479__5__0.0__-99.932__368__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  for qubit_i in range(nq - 1):
    qc.cx(0, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
    qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__0000__2.475282__1__0.0__-0.0002__36__COBYLA
4__2__2__True__0111__4.448608__2__-0.51__-2.8425__66__COBYLA
4__2__2__True__0111__7.366735__3__-0.51__-3.1084__109__COBYLA
4__2__2__True__1111__37.434422__4__0.0__-2.4335__117__COBYLA
4__2__2__True__0111__11.375565__5__-0.51__-3.092__163__COBYLA
16__4__4__True__1111111111111111__5.174341__1__0.0__100000.0__56__COBYLA
16__4__4__True__1111111101111110__9.24383__2__-1.91__-11.7094__110__COBYLA
16__4__4__True__1111111101110111__15.538585__3__-1.32__-14.472__180__COBYLA
16__4__4__True__1111111101001001__18.52747__4__-0.67__-9.1374__211__COBYLA
16__4__4__True__1111111111110110__22.255493__5__-1.97__-15.4308__251__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__4.448934__1__0__0__43__COBYLA
64__8__8__True__1111111111101111111111111111111111111111111111111111111111111111__17.250222__2__1.56__-54.7572__165__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111011101111__23.447777__3__-1.68__-103.2949__220__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__31.021882__4__0.0__-97.6512__293__COBYLA
64__8__8__True__1111111111111111110111111111111111111111111111111111111111111111__46.99091__5__-0.79__-101.1339__433__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  for qubit_i in range(nq - 1):
    qc.cx(0, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
    qc.rz(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__1111__2.548681__1__0.0__0.0004__37__COBYLA
4__2__2__True__1111__3.921338__2__0.0__-0.0057__58__COBYLA
4__2__2__True__0001__5.562922__3__1.35__-0.0001__84__COBYLA
4__2__2__True__1001__6.373918__4__0.84__0.0031__94__COBYLA
4__2__2__True__1001__8.681253__5__0.84__0.0083__125__COBYLA
16__4__4__True__1100011000011001__4.394042__1__3.89__0.0355__53__COBYLA
16__4__4__True__1100110011110000__8.246418__2__1.56__0.049__98__COBYLA
16__4__4__True__0011111100100101__9.901449__3__1.37__-0.0282__117__COBYLA
16__4__4__True__0101110001111011__14.481888__4__2.93__0.01__170__COBYLA
16__4__4__True__0111000101000001__16.512421__5__2.77__-0.0103__191__COBYLA
64__8__8__True__0110101100010101010001101011011100000110000110100011010011100010__6.700092__1__29.62__-0.0393__65__COBYLA
64__8__8__True__1111000001111001010110110110110101101011001000010110011011110000__11.172615__2__16.82__-0.1961__109__COBYLA
64__8__8__True__0011000001000000100100101110001001000111011111111111001100000111__19.942335__3__20.1__0.2272__190__COBYLA
64__8__8__True__1100011000000111001101101001101110001001011010100000110101100100__23.077233__4__24.97__0.2004__221__COBYLA
64__8__8__True__1110110001101010111001000011010010001101011000100111011010011110__26.982814__5__28.26__0.0185__254__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  for qubit_i in range(nq - 1):
    qc.cx(0, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
    qc.rx(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__0100__2.442637__1__1.46__-0.0006__36__COBYLA
4__2__2__True__1011__3.913651__2__1.46__-0.0012__60__COBYLA
4__2__2__True__0010__6.029374__3__-0.62__0.0146__90__COBYLA
4__2__2__True__0000__31.544266__4__0.0__0.0053__104__COBYLA
4__2__2__True__0100__10.15737__5__1.46__-0.0031__149__COBYLA
16__4__4__True__1010000010110010__4.173599__1__4.27__0.0348__50__COBYLA
16__4__4__True__1101000010110100__7.461422__2__2.15__-0.0055__88__COBYLA
16__4__4__True__1100100101111111__10.481596__3__2.35__-0.0208__125__COBYLA
16__4__4__True__0100110001111001__14.933471__4__2.34__0.0627__171__COBYLA
16__4__4__True__0001010000101001__18.729874__5__1.72__0.0283__209__COBYLA
64__8__8__True__1011110001010010110011101000001100100010000000010111111110011100__7.20798__1__25.92__0.0555__68__COBYLA
64__8__8__True__0111010110011001101001100111101110001000001000100001011000101111__13.376487__2__27.48__0.0368__126__COBYLA
64__8__8__True__0100111100011100110010001011100000110001111011001110011100000101__17.483056__3__21.78__-0.1817__169__COBYLA
64__8__8__True__0111001110111100000011011000010011110000011000011011100111010010__23.997974__4__28.08__0.177__229__COBYLA
64__8__8__True__0110111111010110111001000100000001111100000111011010011101110011__27.01022__5__23.07__-0.1408__256__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

for layer_i in range(n_layers):
  for qubit_i in range(nq - 1):
    qc.cx(0, (qubit_i + 1)%nq)
  if layer_i%2:
    for qubit_i in range(nq):
      qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)
  else:
    for qubit_i in range(nq):
      qc.rx(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__1001__2.45407__1__0.84__-0.0007__36__COBYLA
4__2__2__True__1111__4.689737__2__0.0__-1.6594__70__COBYLA
4__2__2__True__1111__34.702487__3__0.0__-2.4315__130__COBYLA
4__2__2__True__1111__8.583628__4__0.0__-1.6729__126__COBYLA
4__2__2__True__0101__12.703848__5__0.77__-2.3876__179__COBYLA
16__4__4__True__0000110001011011__4.233267__1__-0.58__0.0157__49__COBYLA
16__4__4__True__1111111111111111__10.083705__2__0.0__-10.0781__115__COBYLA
16__4__4__True__1111111111110111__12.914663__3__-1.25__-10.8711__148__COBYLA
16__4__4__True__1111111111111111__17.51504__4__0.0__-9.9994__197__COBYLA
16__4__4__True__1111111111110110__29.350482__5__-1.97__-15.3965__329__COBYLA
64__8__8__True__1100110101010100111100001001001110011111111110010011110101110110__6.709841__1__22.4__-0.4956__66__COBYLA
64__8__8__False__0000000000000000000000000000000000000000000000000000000000000000__8.825776__2__0__0__85__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__25.700559__3__0.0__-102.605__237__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__32.077817__4__0.0__-102.62__289__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__45.54352__5__0.0__-102.4885__404__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------

if layer_i%2:
  for qubit_i in range(nq - 1):
    qc.cx(0, (qubit_i + 1)%nq)
  for qubit_i in range(nq):
    qc.ry(theta[(nq*layer_i)+qubit_i], qubit_i)
else:
  for qubit_i in range(nq):
    qc.rx(theta[(nq*layer_i)+qubit_i], qubit_i)

4__2__2__True__1101__2.333889__1__-0.62__-0.0111__35__COBYLA
4__2__2__True__1111__4.407955__2__0.0__-1.6749__65__COBYLA
4__2__2__True__1111__7.405558__3__0.0__-1.6796__109__COBYLA
4__2__2__True__0111__49.351549__4__-0.51__-2.5812__157__COBYLA
4__2__2__True__0111__12.383142__5__-0.51__-3.1223__182__COBYLA
16__4__4__True__0111001110010011__4.800969__1__1.1__-0.0057__52__COBYLA
16__4__4__True__1111111111111111__9.502618__2__0.0__-10.0795__111__COBYLA
16__4__4__True__1111111111111111__12.996651__3__0.0__-10.0964__150__COBYLA
16__4__4__True__1111111111101111__17.531955__4__-0.58__-11.7507__202__COBYLA
16__4__4__True__1111111101101111__23.45298__5__-1.77__-13.5003__270__COBYLA
64__8__8__True__0110011011000010011000010111000101110111011111001101001001111011__6.323463__1__18.26__-0.3363__61__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__18.089854__2__0.0__-102.5958__172__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__26.666918__3__0.0__-102.6003__249__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__30.333616__4__0.0__100000.0__284__COBYLA
64__8__8__True__1111111111111111111111111111111111111111111111111111111111111111__63.93257__5__0.0__-102.4285__379__COBYLA

------------------------------------------------------------------------------------------------------------------------------------------------------------


4__2__2__True__0111__18.098605__6__-0.51__-3.0391__258__COBYLA
4__2__2__True__0111__14.211376__7__-0.51__-3.2088__206__COBYLA
4__2__2__True__1000__23.364609__8__-0.51__-0.4998__339__COBYLA
4__2__2__True__1101__20.595306__9__-0.62__-3.54__297__COBYLA
4__2__2__True__1101__36.794358__10__-0.62__-3.4431__370__COBYLA
4__2__2__True__1101__24.307875__11__-0.62__-3.3246__340__COBYLA
4__2__2__True__1101__24.387085__12__-0.62__-3.5399__345__COBYLA
4__2__2__True__0111__30.400683__13__-0.51__-3.21__430__COBYLA
4__2__2__True__1101__29.267163__14__-0.62__-3.5388__419__COBYLA
4__2__2__True__1101__32.450442__15__-0.62__-3.5382__456__COBYLA
4__2__2__True__0111__35.997592__16__-0.51__-3.2096__511__COBYLA
4__2__2__True__1101__30.495722__17__-0.62__-3.5323__429__COBYLA
4__2__2__True__1101__40.269872__18__-0.62__-3.54__551__COBYLA
4__2__2__True__0111__35.551172__19__-0.51__-3.2069__492__COBYLA
4__2__2__True__1101__43.893364__20__-0.62__-3.5354__607__COBYLA
16__4__4__True__1111011011100011__29.680703__6__-1.46__-12.7121__343__COBYLA
16__4__4__True__1111001000101010__30.971262__7__-1.34__-10.9834__355__COBYLA
16__4__4__True__1111111101111010__39.679021__8__-2.04__-12.7239__452__COBYLA
16__4__4__True__1111001010100111__41.900319__9__-3.97__-14.6207__474__COBYLA
16__4__4__True__1111111101110110__60.10293__10__-2.04__-14.8468__674__COBYLA
16__4__4__True__1111001110110110__62.947434__11__-3.11__-12.5478__702__COBYLA
16__4__4__True__1111001010100111__56.739072__12__-3.97__-15.6577__630__COBYLA
16__4__4__True__1100110101011000__88.967041__13__-2.92__-9.0776__990__COBYLA
16__4__4__True__1111111101111010__88.079531__14__-2.04__-15.0038__967__COBYLA
16__4__4__True__1111111101110110__101.986899__15__-2.04__-16.0468__1110__COBYLA
16__4__4__True__1111111101110110__107.343847__16__-2.04__-17.2921__1160__COBYLA
16__4__4__True__1111111101100111__88.018044__17__-1.9__-15.93__946__COBYLA
16__4__4__True__1111001010100111__87.18327__18__-3.97__-15.1996__932__COBYLA
16__4__4__True__1111111101110110__106.565045__19__-2.04__-17.0472__1130__COBYLA
16__4__4__True__0000111101110110__98.015794__20__-1.08__-9.1965__1037__COBYLA
64__8__8__True__1111111011111111111111111111111111111111111111110000001000000000__46.653834__6__1.24__100000.0__435__COBYLA
64__8__8__True__1101111111011111110111011101110111111111111111111111111111111111__62.597155__7__4.8__-80.9134__583__COBYLA
64__8__8__True__1111110011111111101111110111111111111111111111111101110011111111__77.900036__8__5.33__-75.3222__711__COBYLA
64__8__8__True__1101111111011010000110110001101110111011101110111111111110011010__67.437884__9__10.94__-36.7538__620__COBYLA
64__8__8__True__0000111011011100110011001111111101111111111111111111111011001100__76.412683__10__4.11__-65.6411__697__COBYLA
64__8__8__True__0110110111111011111111111111111111111111111110110001111011101110__86.566115__11__4.81__-73.5512__774__COBYLA
64__8__8__True__1011100011101111111001110111111101111111001001010111110001001111__88.270725__12__14.22__-39.2254__794__COBYLA
64__8__8__True__1000100111011111110111111110111101111111010011111111101001001111__105.998863__13__9.43__-55.557__945__COBYLA
64__8__8__True__0001111011101100110001101111110111111111111100111111111101010110__115.265814__14__10.09__-52.7007__931__COBYLA
64__8__8__True__1111110011111111110111101111111111111111111111111101111111110111__151.076544__15__2.88__-80.2659__1253__COBYLA
64__8__8__True__1111110111110111111111111111111111110111111111111111111011011111__165.952636__16__5.34__100000.0__1343__COBYLA
64__8__8__True__1110101011100111110100011100011111100011111110111101111001101111__171.455654__17__7.57__-46.9787__1397__COBYLA
64__8__8__True__1110101111101110110111001111111011110100101100101111111010101101__9226.255771__18__10.3__-51.519__1614__COBYLA
64__8__8__True__1111110011011111110111111111100001111111111110110110110001111100__196.913739__19__9.15__-55.1829__1680__COBYLA
64__8__8__True__1111101110111110110111111111111111111011111100111101111101100010__196.325902__20__3.25__-66.5857__1653__COBYLA
